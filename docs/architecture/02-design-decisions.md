<\!-- MosaicFS Architecture · ../architecture.md -->

## Design Decisions

| Decision | Rationale |
|---|---|
| **Rust for agent & VFS layer** | The agent and virtual filesystem backends are filesystem daemons where memory safety bugs can cause data corruption. Rust eliminates this class of bugs at compile time. The primary developer has a C/C++ systems background and is already comfortable with Rust, making it the natural choice over Go (which would require learning a new language while building a complex system). |
| **React + Vite for web UI** | The developer is an experienced systems programmer, not a frontend specialist, and will rely on AI tooling to generate and maintain the UI. React has the largest corpus of training data among AI models, producing more reliable code generation than newer frameworks like Svelte. Vite provides fast hot-reload during UI iteration. |
| **CouchDB + PouchDB for sync** | The replication problem between agents and the control plane is a solved problem in CouchDB. Offline-first operation, conflict detection, and incremental sync come for free. The live changes feed enables real-time updates in both the VFS layer and the browser UI without custom WebSocket infrastructure. |
| **UUID-keyed cache with per-transfer integrity** | The VFS file cache keys entries by `file_uuid` (from the file document's `_id`), invalidated when `mtime` or `size` changes in the file document. This eliminates the need to compute a content hash for every file. Because file identity is location-independent, the cache survives file migration between nodes without invalidation. Transfer integrity is provided by an HTTP `Digest` trailer (RFC 9530, `sha-256`) on full-file responses — the serving agent computes the hash as it streams and appends it as a trailer; the receiving agent verifies after the stream completes. Range responses (HTTP 206) do not carry a `Digest` trailer and rely on TLS for in-transit integrity. This cleanly separates file identity (UUID + mtime + size) from transfer integrity (per-response digest), avoids storing a property on the file document that is expensive to compute and only used by the cache, and handles the streaming-hash problem by using trailers rather than headers. |
| **`mosaicfs_browser` read-only CouchDB user for browser sync** | The browser is a larger attack surface than an agent — it can be compromised by XSS, a malicious extension, or a hijacked session in ways that a daemon process cannot. Rather than proxying the full CouchDB replication protocol to an authenticated browser session (which would allow push access to the database), the control plane creates a restricted CouchDB role at setup time with read-only access to a scoped document set. The Axum login endpoint issues a short-lived session token for this role alongside the JWT. Push attempts are rejected by CouchDB's own permission model, not by filter logic that could be misconfigured. A compromised browser session can read indexed file metadata but cannot modify rules, disable credentials, or corrupt the database. |
| **Sorted interval list for block map** | The block cache tracks which regions of a large file are present using a sorted list of non-overlapping `[start, end)` block intervals rather than a raw bitmap. For the home media use case — a user watching a video with a few seeks — this produces 3–15 intervals regardless of file size, serializes to ~40–120 bytes in SQLite, and makes the "find missing ranges to fetch" operation natural. A raw bitmap is O(n) in file size; the interval list is O(k log k) in the number of intervals. A roaring bitmap is a suitable future upgrade for workloads with highly fragmented random access (e.g. large database files), but is unnecessary for video and audio streaming. |
| **Directories own their mount rules** | Rather than maintaining a separate collection of rule documents evaluated against every file, each virtual directory carries a `mounts` array describing what gets mounted inside it. This makes the directory the natural owner of its configuration — editing a directory's mounts is done by navigating to it and changing its properties, not by finding and modifying an abstract rule elsewhere. It also eliminates the global priority ordering problem: mount priorities are local to one directory's `mounts` array, not a system-wide ranking. A file can appear in multiple directories simultaneously, which is an explicit feature rather than a conflict to resolve. |
| **On-demand rule evaluation, no background worker** | Rules are evaluated when a directory is accessed (`readdir`), not pre-computed and stored on file documents. This means rule changes take effect on the next directory listing rather than after a background recomputation cycle. The VFS layer caches `readdir` results for a short TTL to avoid re-evaluating mounts on every `lookup`, invalidated by the PouchDB live changes feed. The tradeoff is that `readdir` is slightly more expensive than reading pre-indexed values, but at home-deployment scale this is imperceptible. |
| **`virtual_path` not stored on file documents** | A file's location in the virtual tree is a derived property, not an intrinsic one. Not storing it acknowledges this honestly and avoids a class of staleness bugs where stored virtual paths outlive the rules that generated them. It also enables the multiple-appearances feature naturally — a file that belongs in three directories has no single canonical virtual path to store. Search operates on intrinsic file properties in v1; virtual-path-aware search can be added later if needed. |
| **Labels stored separately from file documents** | Labels are user-defined metadata that must survive the agent crawler rewriting file documents on every re-crawl. Storing labels on the file document would require the crawler to merge existing labels on every write — a fragile, stateful operation. Separate `label_assignment` documents keyed by `file_uuid` are never touched by the crawler, making the invariant simple and the crawler stateless. Label rules are separate documents rather than embedded in virtual directories because labels are a cross-cutting concern — a label rule should apply to a file everywhere it appears, not only when accessed through a particular directory. The per-file JOIN cost of computing effective labels from two separate document types is eliminated by the materialized label cache — an in-memory hash map maintained incrementally via the CouchDB changes feed. |
| **Plugin filesystem directory as the security boundary** | Plugin configurations are stored in CouchDB and replicated to agents — any user with write access to the database can create or modify a plugin document. Allowing the `plugin_name` field to be an arbitrary command path would make CouchDB write access equivalent to remote code execution on every agent. Restricting execution to a fixed, admin-controlled directory (`/usr/lib/mosaicfs/plugins/` on Linux) means the database controls which plugins are *configured*, while the filesystem controls which plugins are *permitted to run*. Installing a plugin binary requires local admin access to the machine; this is the appropriate trust boundary. Path traversal in `plugin_name` is rejected as a permanent error. |
| **Plugin config in CouchDB, not agent.toml** | Storing plugin configuration in the database rather than in per-machine config files enables web UI management, live reloading without agent restarts, and consistent configuration across a fleet without SSH access to individual machines. The agent watches the changes feed for its own node's plugin documents and reloads configuration within seconds of a change. The only local configuration required is the presence of the plugin binary itself in the plugin directory. |
| **Separate SQLite job queue for plugins** | Plugin jobs need durability across agent restarts — a queue in memory would lose pending jobs on crash. The VFS cache already uses a SQLite sidecar for the block index; a separate `plugin_jobs.db` uses the same infrastructure without mixing concerns. Keeping the plugin queue in a separate file means the cache and the plugin runner can be developed, backed up, and debugged independently. |
| **Full sync as the crash recovery mechanism** | Rather than implementing complex exactly-once delivery guarantees for the event stream, the system accepts that crashes can create gaps and provides a user-triggered full sync as the recovery path. The full sync is idempotent — it compares `annotation.annotated_at` against `file.mtime` and skips files that are already current. This is also the natural onboarding mechanism for newly installed plugins, which need to process all historical files. The tradeoff is that recovery requires a user action rather than being fully automatic, which is acceptable for a home deployment. |
| **Capability-based query routing, plugin identity in the response** | The browser does not know what plugins are installed or which nodes run them. It sends a query with a capability name (`"search"`) to `POST /api/query` and receives an array of labelled result envelopes. The control plane fans the query to all nodes advertising that capability in their `node.capabilities` array; each response envelope carries the plugin's `description` so the UI can label each result section meaningfully. This means adding a new search backend — semantic search, OCR search — requires only deploying a new plugin binary and declaring a `query_endpoints` entry in its plugin document. No control plane code changes, no UI code changes. The UI degrades gracefully when no nodes advertise a capability, rendering that result section simply absent. |
| **Plugin-agent as a standard node in Docker Compose** | Query-serving plugins (fulltext search, semantic search) need to run on a machine with access to the Compose internal network so they can reach services like Meilisearch by hostname. Rather than introducing a new node kind or a special control plane plugin host concept, a standard MosaicFS agent running in the Compose stack satisfies this requirement without any changes to the data model or agent binary. It registers as a regular node with no watch paths and no VFS mount. The only operationally meaningful difference is that it has no `storage` array and `vfs_capable` is false — properties the UI already handles for nodes without watch paths. The indexing plugin (one per physical agent, writes to Meilisearch on file events) and the query plugin (one on the plugin-agent node, reads from Meilisearch on user queries) are two separate binaries sharing the same external service, cleanly separating data pipeline concerns from query concerns. |
| **Plugin settings schema declared in the plugin document** | Plugins declare their user-configurable settings as a JSON Schema-subset object in `settings_schema`. Values entered by the user are stored in a separate `settings` field on the same document. The agent merges `settings` into `config` at invocation time — the plugin binary receives one flat `config` object regardless of which keys came from which source. This separation means the raw `config` field remains available for advanced users and scripted deployments while `settings_schema` enables the UI to render a proper form without any plugin-specific UI code. Supported field types — `string`, `number`, `boolean`, `enum`, `secret` — cover the common configuration surface area for plugins. `secret` fields are stored as plaintext in CouchDB (the database is not externally accessible) but displayed only as `••••••••` in the UI after initial entry, preventing casual observation. |
| **`notification` as a first-class document type with stable deduplication keys** | Notifications from agents, storage backends, the control plane, and plugins all share a single document type rather than being surfaced only through `agent_status` error arrays. This gives the browser a single PouchDB query to watch for all system events, enables real-time delivery via the live changes feed without polling, and allows the UI to show a unified notification bell across all pages. The deterministic `_id` scheme — `notification::{source_id}::{condition_key}` — means a recurring condition is an upsert rather than an accumulation of duplicates. Separating `first_seen` from `last_seen` and tracking `occurrence_count` gives the UI enough information to say "this condition has occurred 47 times since Feb 14" without storing a full event log per notification. Auto-resolving notifications (written by the source when the condition clears) keeps the active notification set clean without requiring manual user intervention for transient issues. |
| **Source-mode storage backends unify data-source adapters with the existing agent model** | Rather than introducing a separate node kind or a dedicated bridge process, external data sources (email, calendar, cloud APIs) are modeled as storage backends in source mode, hosted on standard agents with no watch paths and a `provides_filesystem` plugin acting as their filesystem implementation. This means agents hosting source-mode backends participate in the same document model, replication flows, health monitoring, notification system, and plugin infrastructure as physical nodes — no new code paths for the control plane or browser. |
| **Plugin materialize via VFS cache staging path** | Filesystem-providing plugins that use aggregate storage (Option B) materialize files on demand by writing to `cache/tmp/` — the same staging directory used for Tier 4 remote downloads. The agent moves the staged file into the VFS cache using the standard atomic rename and path-keyed cache entry, then serves from cache. This means all subsequent accesses hit the cache without plugin involvement, range requests and LRU eviction work identically to remote files, and the plugin implementation is trivial — write bytes to a path, return the size. No new streaming protocol, no in-process byte handling, no Digest trailer to compute. The cache `source` column distinguishes plugin-materialized entries from remote downloads for diagnostic purposes only. |
| **DELETE /api/system/data gated by developer mode** | Database wipes are dangerous in production and should require destroying the Docker Compose stack. But during development and testing, being able to quickly cycle between backup/restore states via an API call is valuable. The `--developer-mode` flag on the control plane binary gates access to `DELETE /api/system/data` — enabled for development workflows, disabled by default for production safety. The web UI never exposes this operation; it's API-only and requires a confirmation token in the request body. The intended use is scripted integration tests and local development, not production operation. |
| **Plugin health checks via pull, not push** | Socket plugins are polled for health status on a configurable interval rather than being given a mechanism to push notifications at arbitrary times. The agent's existing heartbeat loop provides the natural cadence; the health check message is a small addition to the socket protocol. This is simpler than managing unsolicited inbound messages on the socket in v1, tolerates the latency of a polling interval (acceptable for operational health reporting), and the socket remains available for a future push extension — an unsolicited `{ "type": "notification" }` message from the plugin would require only a small addition to the inbound message handler without changing the document model or notification lifecycle. |
| **Parent controls step inheritance** | The `enforce_steps_on_children` flag lives on the parent directory, not on child mounts. A parent that sets this flag prepends its steps to every mount evaluation in all descendant directories. Children cannot opt out. This matches how filesystem permissions feel to users — a parent sets policy for its subtree. A child can always add further restrictions on top; it cannot bypass what the parent has decided. |
| **64-bit only** | Inode numbers are random 64-bit integers assigned at document creation time. Supporting 32-bit platforms would reduce the inode space to 32 bits, making collisions a real concern at scale. A compile-time error on 32-bit platforms is cleaner than a silent correctness problem. |
| **Inodes stored in CouchDB** | Storing inode numbers in the database rather than a local sidecar ensures that all nodes running any VFS backend see the same inode for the same file. This makes tools that cache by inode (editors, build systems, backup tools) behave correctly across machines. The inode concept is used directly by FUSE and maps to equivalent identity concepts in macOS File Provider and Windows CFAPI. |
| **Read-only virtual filesystem in v1** | Write support introduces a large class of problems: conflict resolution when the same file is modified on two nodes, ordering of writes through the cache, propagation delays. Deferring this to a later version lets the initial implementation focus on correctness of the read path. All three planned OS backends (FUSE, macOS File Provider, Windows CFAPI) are read-only in v1. |
| **HMAC request signing** | Agent-to-server authentication uses HMAC-SHA256 request signing with a timestamp to prevent replay attacks. This is well-understood, stateless, and requires no session management on the server. The AWS Signature V4 convention was chosen as the naming model because it is familiar to technical users. |
| **Tiered file access** | The common VFS layer tries access methods in order of increasing cost: local file → network mount (CIFS/NFS) → locally-mounted cloud sync → remote HTTP fetch. This logic is shared across all OS-specific backends and ensures that files are served via the cheapest available path without the user having to configure anything beyond declaring what network mounts exist. |
| **Preshared keys for v1** | A single-user home deployment does not need per-resource permissions or multi-user access control. Preshared key pairs (styled after AWS access keys) are simple to implement, well-understood, and sufficient for the intended deployment scenario. The credential document schema is designed to accommodate scoped permissions in a future version. |
| **API-first architecture** | All functionality is implemented as REST API endpoints before any client is built. This ensures that the CLI, web UI, and file browser are true equals — no client has privileged access to functionality unavailable to the others. It also means automation and third-party integrations are possible without special support. |
| **Web UI as primary interface** | The web interface is the recommended management surface for most users. It is served by the control plane, requires no installation, and works on any device with a browser — including tablets where a VFS backend and the agent cannot run. The PWA capability makes it installable on iPad for a near-native experience. |
| **Tauri for desktop file browser** | Tauri wraps the same React frontend used by the web interface, avoiding a separate UI codebase. It uses the system webview rather than bundling Chromium, making the binary significantly smaller and lighter than an Electron equivalent. Native OS integration (file associations, drag-and-drop, system tray) requires a native shell that a web app alone cannot provide. |
| **CLI as API client only** | The CLI carries no daemon functionality and maintains no local state. It is a thin wrapper around the REST API, which means it requires no installation beyond a single binary and no special permissions. It serves as a natural test harness for the API during development and as a scriptable interface for automation. |
| **File browser write operations deferred** | Move, rename, and delete operations in the desktop file browser require a write-capable REST API that does not exist in v1. The virtual filesystem layer is also read-only in v1. Attempting to implement writes in only one client while others remain read-only would create an inconsistent user experience. Write support is planned as a cohesive v2 feature covering the API, VFS layer, and all clients simultaneously. |
| **Filename search in v1, richer search deferred** | Filename and virtual path search is cheap — it operates entirely on data already in CouchDB with no additional infrastructure. Full-text content search requires a separate indexing pipeline, extraction of file contents from remote nodes, and a dedicated search engine (Meilisearch or Tantivy). The complexity and resource cost of content search is out of scope for v1. Metadata filtering (type, size, date, node) and content search are planned for future versions. |
| **Federation over multi-tenancy** | Multi-user support within a single instance requires rearchitecting replication, the rule engine, and every API endpoint to enforce per-user access control. Federation sidesteps this by keeping each instance single-user and making cross-user sharing an explicit, opt-in boundary between sovereign instances. The security model stays simple within each instance; complexity lives at the federation layer, which is optional and additive. |
| **Unified `export_path` for all source types** | Rule sources originally used `real_path`, implying a filesystem path on a physical machine — a concept that doesn't apply to source-mode storage backends or federated peers. Renaming to `export_path` makes the field honest for all node types: filesystem path for physical agents, cloud service path for source-mode backends, and virtual path for federated peers. This gives the rule engine a single code path for resolving sources regardless of node type, and makes merge rules that span local nodes, source-mode storage backends, and federated peers expressible with a uniform schema. |
| **Unified storage backends** | Cloud service bridges and replication targets were separate concepts that both talk to remote storage services — bridges for indexing, targets for replication. Unifying them into a single "storage backend" abstraction means adding a new cloud service requires one implementation, not two. The `hosting_node_id` field replaces `node_kind` as the routing mechanism: set it for platform-locked backends, omit it for network-accessible services. Any agent can host a storage backend, removing the requirement that bridges run on the control plane. |

---

